---
title: "HW1"
author: '[Rui Wang]'
date: "Due Monday September 11, 2017"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
# add other libraries here
```

This exercise involves the Auto data set from ISLR.  Load the data and answer the following questions adding your code in the code chunks. Please submit a pdf version to Sakai.  For full credit, you should push your final Rmd file to your github repo on the STA521-F17 organization site.

```{r data, echo=F}
data(Auto)
```

## Exploratory Data Analysis
1. Create a summary of the data.  How many variables have missing data?

```{r}
summary(Auto)
any(is.na(Auto))
```
#### Answer to Q1 
The summary of dataset Auto is created above. Based on the result, there is no missing data in any variable.

2.  Which of the predictors are quantitative, and which are qualitative?
```{r}
split(names(Auto),sapply(Auto,function(x) paste(class(x),collapse="")))
```
#### Answer to Q2
The results above show that predictor "name" is a factor, which is qualitative. Predictors "mpg","cylinders","displacement","horsepower","weight","acceleration","year" and "origin" are numeric, which should be quanlitative. However, the predictor "origin" should be categorical based on my observation. 

3. What is the range of each quantitative predictor? You can answer this using the `range()` function.   Create a table with variable name, min, max with one row per variable.   `kable` from the package `knitr` can display tables nicely.

```{r}
library(knitr)
Auto_quan=subset(Auto[,1:8])
min_max=apply(Auto_quan,2,range)
kable(t(as.data.frame(min_max)),col.names=c("min","max"),format='latex')
```

4. What is the mean and standard deviation of each quantitative predictor?  _Format nicely in a table as above_

```{r}
mean=apply(Auto_quan,2,mean)
sd=apply(Auto_quan,2,sd)
mean_sd=cbind(mean,sd)
kable(mean_sd,format='latex')
```

5. Now remove the 10th through 85th observations (try this with `filter` from the `dplyr` package). What is the
range, mean, and standard deviation of each predictor in the
subset of the data that remains?  _Again, present the output as a nicely formated table_

```{r}
library(dplyr)
Auto_filter=filter(Auto,! row(Auto)[,1] %in% c(10:85))

### calculate range 
Auto_quan_filter=subset(Auto_filter[,1:8])
min_max_filter=apply(Auto_quan_filter,2,range)
kable(t(as.data.frame(min_max_filter)),col.names=c("min_filter","max_filter"),format='latex')


### calculate mean and standard deviation
mean_filter=apply(Auto_quan_filter,2,mean)
sd_filter=apply(Auto_quan_filter,2,sd)
mean_sd_filter=cbind(mean_filter,sd_filter)
kable(mean_sd_filter,format='latex')
                    
```

6. Investigate the predictors graphically, using scatterplot matrices  (`ggpairs`) and other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your findings.  _Try adding a caption to your figure_

```{r}
library(GGally)
 ggpairs(Auto,columns=1:8)
 
```

#### Answer to Q6
As we can see from the plot above, the correlation between "displacement" and "cylinders" is 0.951, the correlation between "weight" and "displacement" is 0.933. It means that those two pairs of predictors are highly correlated. Putting them altogether in a model may have the problem of multicollinearity. 
Some predictors are positively related such as "displacement" and "cylinders","weight" and "displacement".Some predictors are negatively related such as "weight" and "mpg", "displacement" and "mpg".
In addtion, some predictors may not have a strong relationship such as "cylinders" and "year".

7. Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables using regression. Do your plots suggest that any of the other variables might be useful in predicting mpg using linear regression? Justify your answer.

#### Answer to Q7
According to the plot in Q6, "displacement","horsepwer" and "weight" might be useful in predicting mpg using linear regression. The correlation between "mpg" and "displacement","horsepwer" and "weight" are -0.805,-0.778 and -0.832 resepctively, which indicate strong relationship.


## Simple Linear Regression

8.  Use the `lm()` function to perform a simple linear 
regression with `mpg` as the response and `horsepower` as the
predictor. Use the `summary()` nction to print the results.
Comment on the output.
```{r}
lm_reg=lm(mpg ~ horsepower, data=Auto)
summary(lm_reg)

```

#### Answer to Question 8
(a) Is there a relationship between the predictor and the response?

Answer: Yes, there is. The correlation between the predictor and the response is -0.15784.

(b) How strong is the relationship between the predictor and the response?

Answer: The significance of p-value shows that the relationship between the predicrtor and the response is strong.

(c) Is the relationship between the predictor and the response positive or negative?

Answer: The relationship is negative.

(d)  Provide a brief interpretation of the parameters that would suitable for discussing with a car dealer, who has little statistical background.

Answer: The correation between "horsepower" and "mpg" is negative, which means that a car with larger horsepower spend more gallon per mile than a car with small horsepower. t-value together with p-value indicate that this relationship is very strong.
    
(e) What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?   (see `help(predict)`) Provide interpretations of these for the cardealer.

Answer:The predicted mpg associated with a horsepower of 98 is 24.46708. The prediction interval is (14.8094, 34.12476), which means that the predicted value of mpg with a horsepower of 98 lie in this interval in 95%. The confidence interval is (23.97308, 24.96108), which means that you would expect 95% of this interval to include the true value of the mean of predicted mpg with a horsepower of 98.


```{r}
new=data.frame(horsepower=98)
### calculate prediction interval
pred_plim <- predict(lm_reg, new, interval = "prediction")
kable(pred_plim,format="latex",caption = "prediction interval")

### calculate confidence interval
pred_clim <- predict(lm_reg, new, interval = "confidence")
kable(pred_clim,format="latex",caption = "confidence interval")

```
    
9. Plot the response and the predictor using `ggplot`.  Add to the plot a line showing the least squares regression line. 
```{r}
library(ggplot2)
ggplot(data=Auto,aes(x=horsepower, y=mpg)) +
  geom_point(color="blue")+
  geom_smooth(method='lm')

```

10. Use the `plot()` function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the model regarding assumptions for using a simple linear regression.  

```{r}
par(mfrow=c(2,2))
plot(lm_reg,ask=F)

```

####Answer to Question 10:
The frist Residuals-fitted figure and third figure both suggest curvature and nonconstant variance, which violate the assumption of zero mean and constant variance.

## Theory



11. Show that the  regression function $E(Y \mid x) = f(x)$ is the optimal 
optimal predictor of $Y$ given $X = x$ using squared error loss:  that is $f(x)$
minimizes $E[(Y - g(x))^2 \mid X =x]$ over all functions $g(x)$ at all points $X=x$.
$$E[(Y-g(x))^2 \mid X=x]= E[Y^2-2Yg(x)+g(x)^2 \mid X=x]
                        = E[Y^2\mid X=x]- 2E(Y\mid X=x)g(x)+g(x)^2
$$
In order to minimize $E[(Y-g(x))^2 \mid X=x]$
$$\frac{d}{dg(x)}=2g(x)-2E(Y\mid X=x)=0
$$
$$g(x)=E(Y\mid X=x)
$$
12. Irreducible error:  
     (a) show  that for any estimator $\hat{f}(x)$ that
$$E[(Y - \hat{f}(x))^2 \mid X = x] = 
\underbrace{(f(x) - \hat{f}(x)))^2}_{Reducible} + \underbrace{\textsf{Var}(\epsilon)}_{Irreducible}
$$
We assume that there is some relationship between Y and X, which can be written in a general form
$$Y = f(x) + \epsilon
$$
Our regression function is $$E[Y \mid X=x] = f(x)$$
for any estimator \(\hat{f}(x)\)
it is easy to show that
$$E[(Y-\hat{Y})^2\mid X=x] = E[(Y-\hat{f}(x))^2 \mid X=x] 
                           = E[(f(x)+ \epsilon- \hat{f}(x))^2]
                           =(f(x)- \hat{f}(x))^2+ Var(\epsilon)
$$
 We can optimize $\hat{f}(x)$ to minimize the reducibale error. Since $\epsilon$ is a random error term which is independent of X, the variance accosiated with the error term is irreducible.
   (b) Show that the prediction error can never be smaller than
 $$E[(Y - \hat{f}(x))^2 \mid X = x] \ge \textsf{Var}(\epsilon)
$$

e.g. even if we can learn $f(x)$ perfectly that the error in prediction will not vanish.   

As we can see from part(a), 
$$E[(Y-\hat{Y})^2\mid X=x]= (f(x)- \hat{f}(x))^2+ Var(\epsilon)
$$
Since 
$$(f(x)- \hat{f}(x))^2  \geqslant 0 
$$
We have 
$$ E[(Y-\hat{f}(x))^2\mid X=x]  \geqslant Var(\epsilon)
$$
